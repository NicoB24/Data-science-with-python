{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# funPyModeling\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#import qgrid\n",
    "#import seaborn as sns\n",
    "#from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_pair(data, method='pearson'):\n",
    "    d_cor=data.corr(method)\n",
    "\n",
    "    d_cor2=d_cor.reset_index() # me genera el index como columna\n",
    "\n",
    "    d_long=d_cor2.melt(id_vars='index') # lo paso a long format, cada reg 1 variable\n",
    "\n",
    "    d_long.columns=['v1', 'v2', 'R']\n",
    "    \n",
    "    d_long[['R2']]=d_long[['R']]**2\n",
    "    \n",
    "    d_long2=d_long.query(\"v1 != v2\")\n",
    "\n",
    "    return(d_long2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_pair(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_pair(data, 'kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status(dat):\n",
    "    if isinstance(dat, pd.Series):\n",
    "        data=pd.DataFrame(dat)\n",
    "    else:\n",
    "        data=dat\n",
    "        \n",
    "    if(isinstance(data, np.ndarray)):\n",
    "        data=pd.DataFrame(data)\n",
    "\n",
    "        \n",
    "    # total de rows\n",
    "    tot_rows=len(data)\n",
    "    \n",
    "    # total de nan\n",
    "    d2=data.isnull().sum().reset_index()\n",
    "    d2.columns=['variable', 'q_nan']\n",
    "    \n",
    "    # percentage of nan\n",
    "    d2[['p_nan']]=d2[['q_nan']]/tot_rows\n",
    "    \n",
    "    # num of zeros\n",
    "    d2['q_zeros']=(data==0).sum().values\n",
    "\n",
    "    # perc of zeros\n",
    "    d2['p_zeros']=d2[['q_zeros']]/tot_rows\n",
    "\n",
    "    # total unique values\n",
    "    d2['unique']=data.nunique().values\n",
    "    \n",
    "    # get data types per column\n",
    "    d2['type']=[str(x) for x in data.dtypes.values]\n",
    "    \n",
    "    return(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "#data=pd.read_csv(\"data/eph2.txt\", sep = \",\")\n",
    "#cat_v = data.select_dtypes(include=['object']).columns\n",
    "#cat_v.drop('sexo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_vars(data, exclude_var=None):\n",
    "    cat_v = data.select_dtypes(include=['object']).columns\n",
    "    if exclude_var is not None: \n",
    "        cat_v=cat_v.drop(exclude_var)\n",
    "    return cat_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_vars(data, exclude_var=None):\n",
    "    num_v = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if exclude_var is not None: \n",
    "        num_v=num_v.drop(exclude_var)\n",
    "    return num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiling_num(data):\n",
    "    # ask if series/array or dataframe\n",
    "    if(len(data.shape)==1):\n",
    "        d=pd.DataFrame({data.name:data})\n",
    "    \n",
    "    # explicit keep the num vars\n",
    "    d=data[num_vars(data)]\n",
    "    \n",
    "    des1=pd.DataFrame({'mean':d.mean().transpose(), \n",
    "                   'std_dev':d.std().transpose()})\n",
    "\n",
    "    des1['variation_coef']=des1['std_dev']/des1['mean']\n",
    "    \n",
    "    d_quant=d.quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).transpose().add_prefix('p_')\n",
    "    \n",
    "    des2=des1.join(d_quant, how='outer')\n",
    "    \n",
    "    des_final=des2.copy()\n",
    "    \n",
    "    des_final['variable'] = des_final.index\n",
    "    \n",
    "    des_final=des_final.reset_index(drop=True)\n",
    "    \n",
    "    des_final=des_final[['variable', 'mean', 'std_dev','variation_coef', 'p_0.01', 'p_0.05', 'p_0.25', 'p_0.5', 'p_0.75', 'p_0.95', 'p_0.99']]\n",
    "    \n",
    "    return des_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_tbl_logic(var, name):\n",
    "    cnt=var.value_counts()\n",
    "    df_res=pd.DataFrame({'frequency': var.value_counts(), 'percentage': var.value_counts()/len(var)})\n",
    "    df_res.reset_index(drop=True)\n",
    "    \n",
    "    df_res[name] = df_res.index\n",
    "    \n",
    "    df_res=df_res.reset_index(drop=True)\n",
    "    \n",
    "    df_res['cumulative_perc'] = df_res.percentage.cumsum()/df_res.percentage.sum()\n",
    "    \n",
    "    df_res=df_res[[name, 'frequency', 'percentage', 'cumulative_perc']]\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def todf(data):\n",
    "    if isinstance(data, list):\n",
    "        data=np.array(data)\n",
    "\n",
    "    if(len(data.shape))>2:\n",
    "        raise Exception(\"I live in flattland! (can't handle objects with more than 2 dimensions)\") \n",
    "\n",
    "    if isinstance(data, pd.Series):\n",
    "        data2=pd.DataFrame({data.name: data})\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        if(data.shape==1):\n",
    "            data2=pd.DataFrame({'var': data})\n",
    "        else:\n",
    "            data2=pd.DataFrame(data)\n",
    "    else: \n",
    "        data2=data\n",
    "        \n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_tbl(data):\n",
    "    data=todf(data)\n",
    "    \n",
    "    cat_v=cat_vars(data)\n",
    "    \n",
    "    if(len(cat_v)>1):\n",
    "        for col in cat_v:\n",
    "            print(freq_tbl_logic(data[col], name=col))\n",
    "            print('\\n----------------------------------------------------------------\\n')\n",
    "        return cat_v\n",
    "    else:\n",
    "        return freq_tbl_logic(data.iloc[:,0], name=data.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_tbl_old(data):\n",
    "    if(len(data.shape)==1):\n",
    "        res=freq_tbl_logic(data, name=data.name)\n",
    "        return res\n",
    "    else:\n",
    "        cat_v=cat_vars(data)\n",
    "        for col in cat_v:\n",
    "            print(freq_tbl_logic(data[col], name=col))\n",
    "            print('\\n----------------------------------------------------------------\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_plot(data, cluster_var):\n",
    "    # 1) Agrupo por cluster, x promedio\n",
    "    x_grp=data.groupby(cluster_var).mean()\n",
    "    x_grp[cluster_var] = x_grp.index \n",
    "    x_grp=x_grp.reset_index(drop=True)\n",
    "    x_grp # data con las variables originales\n",
    "    \n",
    "    # 2) Tengo que hacer minmax de los datos agrupados\n",
    "    x_grp_no_tgt=x_grp.drop(cluster_var, axis=1)\n",
    "\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    mm_scaler.fit(x_grp_no_tgt)\n",
    "    x_grp_mm=mm_scaler.transform(x_grp_no_tgt)\n",
    "\n",
    "    # 3) convertir a df\n",
    "    df_grp_mm=pd.DataFrame(x_grp_mm, columns=x_grp_no_tgt.columns)\n",
    "\n",
    "    df_grp_mm[cluster_var]=x_grp[cluster_var] # variables escaladas\n",
    "\n",
    "    # Make the plot\n",
    "    parallel_coordinates(df_grp_mm, cluster_var, colormap=plt.get_cmap(\"Dark2\"))\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    return [x_grp, df_grp_mm]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
